# -*- coding: utf-8 -*-
"""House_prices_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTr6_nE4lsdB_o6yqxEtBh1ydLXu_kxF

# This notebook aims to predict house prices using the popular dataset 'House Prices - Advanced Regression Techniques' on Kaggle.
## Going to do some exploratory data analysis to start with as always, then will get started on some regression techniques.
### https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
### Some predictions at the end using mlr model and tensorflow neural network.

#### Load libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats

"""#### Load train data and explore"""

train = pd.read_csv('train.csv')

train.head()

train.columns

#Analayse the all important variable, sale price
train['SalePrice'].describe()

sns.distplot(train['SalePrice'])

sns.boxplot(train['SalePrice'])

"""Determine correlation between quantitative variables"""

train.corr()['SalePrice']

"""Assess null values and fill"""

train.isnull().any().head(20)

train_na = (train.isnull().sum() / len(train)) * 100
train_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :train_na})
missing_data.head(20)

f, ax = plt.subplots(figsize=(15, 12))
plt.xticks(rotation='90')
sns.barplot(x=train_na.index, y=train_na)
plt.xlabel('Features', fontsize=15)
plt.ylabel('Percent of missing values', fontsize=15)
plt.title('Percent missing data by feature', fontsize=15)

"""Since na values means 'nothing' for following columns:"""

for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', "PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', "MasVnrType", 'MSSubClass'):
    train[col] = train[col].fillna('None')

"""Fill null values of lot frontage with median for neighbourhood"""

train["LotFrontage"] = train.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

"""Since null values of following columns (likely) means zero"""

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', "MasVnrArea"):
    train[col] = train[col].fillna(0)

"""fill zoning with most common values for column"""

train['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])

"""data description says null likely means typical"""

train["Functional"] = train["Functional"].fillna("Typ")

train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])
train['KitchenQual'] = train['KitchenQual'].fillna(train['KitchenQual'].mode()[0])
train['Exterior1st'] = train['Exterior1st'].fillna(train['Exterior1st'].mode()[0])
train['Exterior2nd'] = train['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])

train['SaleType'] = train['SaleType'].fillna(train['SaleType'].mode()[0])

"""Check any remaining null values"""

train_na = (train.isnull().sum() / len(train)) * 100
train_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :train_na})
missing_data.head()

"""All good

### Some more feature engineerign

Utilities column won't help, therefore removed
"""

train = train.drop(['Utilities'], axis=1)

train = pd.get_dummies(train)

"""#### Train Test Split"""

from sklearn.model_selection import train_test_split

feature_columns = train.columns[train.columns != 'SalePrice']
features = train[feature_columns]

labels = train['SalePrice']

features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)

"""#### Column Transformer to scale data"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import Normalizer

ct = ColumnTransformer([('normalize', Normalizer(), feature_columns)], remainder='passthrough')

features_train_norm = ct.fit_transform(features_train)

features_test_norm = ct.transform(features_test)

features_train_norm = pd.DataFrame(features_train_norm, columns = feature_columns)
features_test_norm = pd.DataFrame(features_test_norm, columns = feature_columns)

"""### Mulitple Linear Regression
#### Simple mlr model using sklearn to get base predictions
"""

from sklearn.linear_model import LinearRegression

mlr = LinearRegression()

model_mlr = mlr.fit(features_train_norm, labels_train)

y_mlr_predict = model_mlr.predict(features_test_norm)

"""Get scores for sklearn linearregression model"""

print("Train score:")
print(model_mlr.score(features_train_norm, labels_train))
print("Test score:")
print(model_mlr.score(features_test_norm, labels_test))

from sklearn.metrics import mean_absolute_error

mlr_mae = mean_absolute_error(labels_test, y_mlr_predict)
mlr_mae

"""Not bad, must be a fair bit of overfitting though"""

plt.scatter(labels_test, y_mlr_predict)
x = range(500000)
y = range(500000)
plt.plot(x, y , color='red')
plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")
plt.title("Actual Rent vs Predicted Rent")

plt.show()

"""Can see greater skew towards the higher end of the price range. This is due to the limited data, and could be improved using gradient boosting.

### Create a regression model using Tensorflow
"""

import tensorflow

from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

"""Create a design model function to create neural network"""

def design_model():
  model = Sequential()
  input = InputLayer(input_shape=(features.shape[1],)) 
  model.add(input) 
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(64, activation = 'relu'))
  model.add(Dropout(0.2))
  model.add(Dense(24, activation = 'relu'))
  model.add(Dense(1)) 
  opt = Adam(learning_rate=0.01)
  model.compile(loss='mse',  metrics=['mae'], optimizer=opt)
  return model

"""Initial fit using random hyperparameters."""

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)

model = design_model()
model.fit(features_train_norm, labels_train, epochs=100, batch_size=16, callbacks=[es])

"""Create a randomised grid search to find best hyperparameters"""

from sklearn.model_selection import RandomizedSearchCV
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error

def do_randomised_search():
  param_grid = {'batch_size': sp_randint(2, 16), 'nb_epoch': sp_randint(50, 200)}
  model = KerasRegressor(build_fn=design_model)
  grid = RandomizedSearchCV(estimator = model, param_distributions=param_grid, scoring = make_scorer(mean_squared_error, greater_is_better=False), n_iter = 12)
  grid_result = grid.fit(features_train, labels_train, verbose = 0)
  print(grid_result)
  print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

  means = grid_result.cv_results_['mean_test_score']
  stds = grid_result.cv_results_['std_test_score']
  params = grid_result.cv_results_['params']
  for mean, stdev, param in zip(means, stds, params):
      print("%f (%f) with: %r" % (mean, stdev, param))

do_randomised_search()

"""Best batch size: 2, best number of epochs: 177."""

model.fit(features_train_norm, labels_train, epochs=177, batch_size=2, callbacks=[es])

tf_predictions = model.predict(features_test_norm)

plt.scatter(labels_test, tf_predictions)
x = range(500000)
y = range(500000)
plt.plot(x, y , color='red')

plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\hat{Y}_i$")
plt.title("Actual Rent vs Predicted Rent")

plt.show()

tf_val_mse, tf_val_mae = model.evaluate(features_test_norm, labels_test, verbose=0)

tf_val_mae

"""Not bad

#### Tensorflow model has a lower mae by about 7000 than mlr model.
"""